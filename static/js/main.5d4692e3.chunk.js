(this.webpackJsonptaxonomy=this.webpackJsonptaxonomy||[]).push([[0],{124:function(e){e.exports=JSON.parse('{"name":"Start","children":[{"name":"Wrangling","children":[{"name":"Transform  numerical features","children":[{"name":"standarize across features","id":300,"level":3,"practice_id":27,"description":"transform/standarize","urls":["https://stackoverflow.com/questions/63231343","","",""]},{"name":"standarize data","id":301,"level":3,"practice_id":28,"description":"transform/standarize","urls":["https://stackoverflow.com/questions/5652357","","",""]},{"name":"scale data","id":302,"level":3,"practice_id":43,"description":"transform/standarize","urls":["https://stackoverflow.com/questions/5652357","","",""]},{"name":"normalize data","id":303,"level":3,"practice_id":20,"description":"transform data (normalize,standarize,scale) based on the algorithms (e.g., model assumptions, model characteristics, model sensitivity to magnitude)  that will use the data and  the data nature (e.g., data distributions, data types) ","urls":["https://stackoverflow.com/questions/5652357","","",""]},{"name":"normalize data based on algorithms","id":304,"level":3,"practice_id":21,"description":"transform data (normalize,standarize,scale) based on the algorithms (e.g., model assumptions, model characteristics, model sensitivity to magnitude)  that will use the data and  the data nature (e.g., data distributions, data types) ","urls":["https://datascience.stackexchange.com/questions/37734 ","https://stats.stackexchange.com/questions/189652","",""]},{"name":"normalize data relu","id":305,"level":3,"practice_id":22,"description":"transform data (normalize,standarize,scale) based on the algorithms (e.g., model assumptions, model characteristics, model sensitivity to magnitude)  that will use the data and  the data nature (e.g., data distributions, data types) ","urls":["https://datascience.stackexchange.com/questions/25832","","",""]},{"name":"normalize data based on algorithm and data","id":306,"level":3,"practice_id":23,"description":"transform data (normalize,standarize,scale) based on the algorithms (e.g., model assumptions, model characteristics, model sensitivity to magnitude)  that will use the data and  the data nature (e.g., data distributions, data types) ","urls":["https://stats.stackexchange.com/questions/189652","","",""]},{"name":"normalize data when model sensitive magnitude","id":307,"level":3,"practice_id":24,"description":"transform data (normalize,standarize,scale) based on the algorithms (e.g., model assumptions, model characteristics, model sensitivity to magnitude)  that will use the data and  the data nature (e.g., data distributions, data types) ","urls":["https://stats.stackexchange.com/questions/189652","","",""]},{"name":"normalize data when using SVM","id":308,"level":3,"practice_id":25,"description":"transform data (normalize,standarize,scale) based on the algorithms (e.g., model assumptions, model characteristics, model sensitivity to magnitude)  that will use the data and  the data nature (e.g., data distributions, data types) ","urls":["https://stats.stackexchange.com/questions/189652","","",""]},{"name":"transform data based on algorithms","id":309,"level":3,"practice_id":30,"description":"transform data (normalize,standarize,scale) based on the algorithms (e.g., model assumptions, model characteristics, model sensitivity to magnitude)  that will use the data and  the data nature (e.g., data distributions, data types) ","urls":["https://datascience.stackexchange.com/questions/65215 ","","",""]},{"name":"scale all numeric features","id":310,"level":3,"practice_id":44,"description":"scale numeric features","urls":["https://datascience.stackexchange.com/questions/89696","","",""]},{"name":"scale data based on algorithms","id":311,"level":3,"practice_id":42,"description":"scale numeric features","urls":["https://datascience.stackexchange.com/questions/89696","","",""]},{"name":"normalize with training statistics","id":312,"level":3,"practice_id":26,"description":"transform data (normalize,standarize,scale) in training, validation and testing sets with the statistics computed from the training set","urls":["https://cs.stackexchange.com/questions/60391","https://stats.stackexchange.com/questions/77350","",""]},{"name":"transform all datasets with training statistics","id":313,"level":3,"practice_id":29,"description":"transform data (normalize,standarize,scale) in training, validation and testing sets with the statistics computed from the training set","urls":["https://datascience.stackexchange.com/questions/75178","https://stats.stackexchange.com/questions/482486","https://cs.stackexchange.com/questions/115441",""]},{"name":"use directional statistics to handle geo data","id":314,"level":3,"practice_id":31,"description":"in the case of handling geo data, transform latitude and longitude by using directional statistics to deal with potential interdependence of lattitute and longitude and scales wrapping ","urls":["https://stats.stackexchange.com/questions/207533","","",""]},{"name":"apply smoothing to deal with infinity ratios","id":315,"level":3,"practice_id":183,"description":"when  transforming numerical data to ratios and some transformations become infinite ( e.g.,  zero or too small denominator), use smoothing (i.e., add a small constant in the dominator to avoid  dividing by zero)","urls":["https://stackoverflow.com/questions/46355357","","",""]}],"id":200,"level":2},{"name":"Transform non numerical data","children":[{"name":"Split image crop object of interest to detect","id":316,"level":3,"practice_id":18,"description":"for object detection in images crop the object of interest instead of using the whole image","urls":["https://stackoverflow.com/questions/40476041","","",""]},{"name":"Split image 2 characters","id":317,"level":3,"practice_id":19,"description":"for analyzing hand written text images split the whole image into individual characters","urls":["https://opendata.stackexchange.com/questions/7225","","",""]}],"id":201,"level":2},{"name":"Transform misellaneous data type","children":[{"name":"pad sequences at the end for LSTM","id":318,"level":3,"practice_id":5,"description":"pad sequences to equalize length when feeding the, to LSTMS","urls":["https://stackoverflow.com/questions/44131718","","",""]},{"name":"anonymize using hashing and salt","id":319,"level":3,"practice_id":40,"description":"when anonimizing data with hashing, use it over a unique identifier with salt","urls":["https://datascience.stackexchange.com/questions/3701","","",""]}],"id":202,"level":2},{"name":"Augment dataset","children":[{"name":"augment data to avoid overfitting","id":320,"level":3,"practice_id":1,"description":"use data augmentation techniques in order to prevent overfitting","urls":["https://stackoverflow.com/questions/32966970","","",""]}],"id":203,"level":2},{"name":"Encode non numerical data","children":[{"name":"encode Categorical multiple binary","id":321,"level":3,"practice_id":11,"description":"if an algorithm does not support categorical data, this type of feature should be encoded multiple binary features (e.g., one-hot encoding), or by counting frequencies","urls":["https://datascience.stackexchange.com/questions/5226","https://stackoverflow.com/questions/19512863","",""]},{"name":"encode categorical to frequency","id":322,"level":3,"practice_id":13,"description":"if an algorithm does not support categorical data, this type of feature should be encoded multiple binary features (e.g., one-hot encoding), or by counting frequencies","urls":["https://stackoverflow.com/questions/12836440","","",""]},{"name":"use cyclical features to encode date/time","id":323,"level":3,"practice_id":36,"description":"represent date time with cyclical characteristic ","urls":["https://stats.stackexchange.com/questions/311494","","",""]},{"name":"encode datetime features with sine/cosine facets","id":324,"level":3,"practice_id":12,"description":"e.g., you could use sine/cosine facets for representing cyclical characteristics from date time features","urls":["https://stats.stackexchange.com/questions/311494","","",""]},{"name":"Split datetime into four features","id":325,"level":3,"practice_id":17,"description":"if you have datetime data you could split  in the corresponding components (e.g., if you have day-month-year then split it into day, month, year) ","urls":["https://stats.stackexchange.com/questions/311494","","",""]}],"id":204,"level":2},{"name":"Encode misellaneous","children":[{"name":"encode datetime features with frequencies or general features","id":326,"level":3,"practice_id":14,"description":"if you want to use features that are dependant on time along with other features, you could encode them as frequencies","urls":["https://stackoverflow.com/questions/18853557","","",""]},{"name":"encode order of a feature if required when clustering is applied","id":327,"level":3,"practice_id":10,"description":"when using clustering and having aggregated structured data (e.g., a 3D array) if in your data the order matters then you should create a new feature to encode the order","urls":["https://datascience.stackexchange.com/questions/65215","","",""]}],"id":205,"level":2},{"name":"Encode numerical data","children":[{"name":"use geodesic distance to predict geo data","id":328,"level":3,"practice_id":33,"description":"In the case of dealing with geo data use geodesic distance to compute distance between two points of interest","urls":["https://stats.stackexchange.com/questions/207566","","",""]}],"id":206,"level":2},{"name":"Impute missing data","children":[{"name":"replicate training imputation on validation and test datasets","id":329,"level":3,"practice_id":6,"description":"if you impute data on training, you should replicate the same imputation procedure on validation and testing data","urls":["https://stats.stackexchange.com/questions/503677","","",""]},{"name":"use imputation techiques when missing data","id":330,"level":3,"practice_id":38,"description":"impute_missing data","urls":["https://datascience.stackexchange.com/questions/39058","https://stackoverflow.com/questions/52600690","https://stats.stackexchange.com/questions/503677",""]}],"id":207,"level":2},{"name":"Balance data","children":[{"name":"balance data","id":331,"level":3,"practice_id":3,"description":"balance data","urls":["https://datascience.stackexchange.com/questions/13901","https://stackoverflow.com/questions/46178141","https://datascience.stackexchange.com/questions/82012","https://stackoverflow.com/questions/40918978"]},{"name":"balance data only on training","id":332,"level":3,"practice_id":4,"description":"If applying any procedure for balancing data you should do it only on the dataset used for training","urls":["https://stackoverflow.com/questions/51064462","https://datascience.stackexchange.com/questions/82012","",""]},{"name":"undersampling randomly to balance data","id":333,"level":3,"practice_id":2,"description":"if you undersample you should select the data in the sample randomly","urls":["https://stackoverflow.com/questions/46178141","","",""]},{"name":"use oversample after samplingdata","id":334,"level":3,"practice_id":37,"description":"If you oversample data you should do it after splitting the dataset (e.g., after splitting into train, validation, test)","urls":["https://stackoverflow.com/questions/51064462","","",""]}],"id":208,"level":2},{"name":"Eliminate noisy data","children":[{"name":"eliminate noisy data","id":335,"level":3,"practice_id":7,"description":"eliminate noisy data (e.g., smoothing)","urls":["https://datascience.stackexchange.com/questions/13901","https://stackoverflow.com/questions/46355357","",""]}],"id":209,"level":2},{"name":"Prepare dataset","children":[{"name":"generate a superset vocabulary","id":336,"level":3,"practice_id":48,"description":"when using bag of words representation for textual data build the vocabulary on the whole dataset","urls":["https://datascience.stackexchange.com/questions/32859","","",""]}],"id":210,"level":2}],"id":100,"level":1},{"name":"Feature_selection","children":[{"name":"Consider existing techniques and their assumptions","children":[{"name":"check pca assumptions hold","id":337,"level":3,"practice_id":52,"description":"when using feature selection algorithms evaluate/check  that the algorithm assumptions hold","urls":["https://datascience.stackexchange.com/questions/17645","","",""]},{"name":"check dimensionality reduction","id":338,"level":3,"practice_id":51,"description":"for selecting features, check the existing techniques of dimensionality reduction","urls":["https://stats.stackexchange.com/questions/256517","","",""]},{"name":"try dimensionality reduction technique","id":339,"level":3,"practice_id":54,"description":"for selecting features, check the existing techniques of dimensionality reduction","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"check different selection techniques","id":340,"level":3,"practice_id":50,"description":"for selecting features check the existing techiques for this purpouse","urls":["https://stackoverflow.com/questions/54034392","https://stats.stackexchange.com/questions/261902","",""]},{"name":"try variable important index given by random forest","id":341,"level":3,"practice_id":59,"description":"e.g, select features based on the feature importance index given by random forest","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"use p value greater than 0.5 in logistic regression","id":342,"level":3,"practice_id":63,"description":"e.g., select significant features (e.g., with a p value smaller than 0.5 in logistic regression)","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"try mutual information","id":343,"level":3,"practice_id":57,"description":"e.g., mutual information feature selection","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"try Backward or forward selection independently any classifier","id":344,"level":3,"practice_id":53,"description":"e.g., select features with backward and forwad selection technique","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"try elastic net when high collinearity","id":345,"level":3,"practice_id":55,"description":"e.g, select features using regularization that is part of the algorithm (e.g, lasso , elastic net when high collinearity)","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"try lasso regularization","id":346,"level":3,"practice_id":56,"description":"e.g, select features using regularization that is part of the algorithm (e.g, lasso , elastic net when high collinearity)","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"try regularization in algorithms","id":347,"level":3,"practice_id":58,"description":"e.g, select features using regularization that is part of the algorithm (e.g, lasso , elastic net when high collinearity)","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"try latent variables","id":348,"level":3,"practice_id":60,"description":"try to use latent variables  (e.g., Partial Least Squares or Canonical Correlation Analysis) instead of the original ones","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"use ROC for evaluating hypotesis for specific feature","id":349,"level":3,"practice_id":62,"description":"use ROC as a univariate feature selection technique when you want to identify the impact of an specifi feature om the model performance","urls":["https://stats.stackexchange.com/questions/261902","","",""]}],"id":211,"level":2},{"name":"General ","children":[{"name":"select features only on train data","id":350,"level":3,"practice_id":49,"description":"when selecting features, select them from the data that is going to be used for training insetd of the whole dataset","urls":["https://stackoverflow.com/questions/54034392","","",""]},{"name":"use multivariate over univariate","id":351,"level":3,"practice_id":64,"description":"when selecting features prefer multivariate feature selection over univariate feature selection ","urls":["https://stats.stackexchange.com/questions/261671","","",""]}],"id":212,"level":2}],"id":101,"level":1},{"name":"EDA","children":[{"name":"Define types of features and  dependencies between them","children":[{"name":"determine depedant and independant variables","id":352,"level":3,"practice_id":8,"description":"when analyzing data determine which variables are dependant and which ones independent","urls":["https://stackoverflow.com/questions/5652357","","",""]},{"name":"determine right type for each feature","id":353,"level":3,"practice_id":9,"description":"when analyzing data identify the type of each feature (e.g., numeric, categorical, time data) before applying a preprocessing technique","urls":["https://stackoverflow.com/questions/5652357","","",""]}],"id":213,"level":2},{"name":"Detect trends, errors  and relations in data","children":[{"name":"identify data errors","id":354,"level":3,"practice_id":65,"description":"before preprocessing data identify possible errors (e.g., format of time data)","urls":["https://datascience.stackexchange.com/questions/2368","","",""]},{"name":"identify time trends","id":355,"level":3,"practice_id":66,"description":"when analyzing time  data, identify possible temporal  trends (e.g., every Monday, every January, every two weeks)","urls":["https://datascience.stackexchange.com/questions/2368","","",""]},{"name":"identify weird trends","id":356,"level":3,"practice_id":67,"description":"when analyzing time data, identify possible weird  trends (e.g, trends related to easter, super bowl)","urls":["https://datascience.stackexchange.com/questions/2368","","",""]},{"name":"understand missing values","id":357,"level":3,"practice_id":68,"description":"before preprocessing  data, identify missing values and understand the nature of them","urls":["https://datascience.stackexchange.com/questions/39058","","",""]},{"name":"cluster geoloccation data to deduce information","id":358,"level":3,"practice_id":47,"description":"cluster geo located data in order to find correlations between a target class and the existing features and instances","urls":["https://datascience.stackexchange.com/questions/64990","","",""]}],"id":214,"level":2}],"id":102,"level":1},{"name":"DATA","children":[{"name":"Improve performance","children":[{"name":"Split dataset for regression when data is dependable on categorical data","id":359,"level":3,"practice_id":16,"description":"split the whole dataset for each category of a categorical feature if you want to construct specific models for each one (e.g., split a dataset based on a continent feature into Asia and Europe splits if you want to predict specific car values in each  continent)","urls":["https://stats.stackexchange.com/questions/191348","","",""]}],"id":215,"level":2},{"name":"Prevent computation of  biased metrics and avoid overfitting","children":[{"name":"split set train validation test","id":360,"level":3,"practice_id":127,"description":"when optimizing hyper parameters you should divide the whole dataset into train, validation and test ","urls":["https://stats.stackexchange.com/questions/179872","","",""]},{"name":"split train test then split train into folds","id":361,"level":3,"practice_id":128,"description":"when optimizing hyper parameters you should divide the whole dataset into train and test, and then divide the train into folds for crossvalidation","urls":["https://stats.stackexchange.com/questions/179872","https://stats.stackexchange.com/questions/342110","https://stats.stackexchange.com/questions/410118",""]}],"id":216,"level":2},{"name":"Dataset construction","children":[{"name":"reflect real distribution in train data","id":362,"level":3,"practice_id":135,"description":"training data should reflect the real distribution of the data","urls":["https://stackoverflow.com/questions/48234558","","",""]},{"name":"select negative sample images that look similar to positive ones","id":363,"level":3,"practice_id":131,"description":"when working with images, the negative samples should look similar to the positive ones (e.g., bus (negative) are similar to cars (positive))","urls":["https://stackoverflow.com/questions/48234558","","",""]},{"name":"use images similar to test images","id":364,"level":3,"practice_id":133,"description":"when  classifying images , the ones used in the training set should be  similar to the images that are going to be used for testing","urls":["https://stackoverflow.com/questions/35915685","","",""]},{"name":"select negative sample images based on the case of use","id":365,"level":3,"practice_id":130,"description":"when working with images, the negative samples should  take into account the usage scenarios. For instance, in an app for a smartphone, then the negative sample should probably include street views, pictures of buildings and stores, people, indoor environment, etc. It\'s unlikely that the image from the smartphone camera will be a wild animal or abstract painting, i.e., it\'s an improbable input in your real distribution .","urls":["https://stackoverflow.com/questions/48234558","","",""]},{"name":"ensure all positive images have to have same aspect ratio","id":366,"level":3,"practice_id":126,"description":"for object detection, it is recommended that the object regions of interest (ROI) have  similar aspect ratio in all the positive images","urls":["https://stackoverflow.com/questions/40476041","","",""]},{"name":"use preexisting datasets to augment negative classes","id":367,"level":3,"practice_id":132,"description":"if needed use pre-existing datasets to augment your negative class ","urls":["https://stackoverflow.com/questions/48234558","","",""]},{"name":"select the minimum size as the minimum to detect","id":368,"level":3,"practice_id":129,"description":"for object detection, the minimum size of the training samples should be the same as the minimum size that is going to be detected","urls":["https://stackoverflow.com/questions/40476041","","",""]}],"id":217,"level":2},{"name":"Ensure minimum size and how to measure the size","children":[{"name":"ensure minimum size in thousands","id":369,"level":3,"practice_id":125,"description":"when building models, make sure that the size of the dataset that you will use is in the order of thousands","urls":["https://datascience.stackexchange.com/questions/12344","","",""]},{"name":"meassure size after preprocessing","id":370,"level":3,"practice_id":123,"description":"measure the size of the dataset  after preprocessing (I.e., you should size up the dataset after deleting all features and registers needed)","urls":["https://datascience.stackexchange.com/questions/13901","","",""]},{"name":"meassure size in terms of rows columns","id":371,"level":3,"practice_id":124,"description":"measure the size of the dataset not only by size (i.e., Mb, Gb), but also meassure it in terms of rows and columns","urls":["https://datascience.stackexchange.com/questions/13901","","",""]}],"id":218,"level":2}],"id":103,"level":1},{"name":"Implementation","children":[{"name":"Reproducibility/replicability","children":[{"name":"use pipelines to enable reproducibility data preprocessing","id":372,"level":3,"practice_id":116,"description":"export and use pipelines to enable reproducibility in data preprocessing","urls":["https://stats.stackexchange.com/questions/400242","https://stats.stackexchange.com/questions/482486","",""]},{"name":"use pipelines","id":373,"level":3,"practice_id":113,"description":"use pipelines to automate processes in order to save time in complex tasks","urls":["https://datascience.stackexchange.com/questions/13901","","",""]},{"name":"reuse existing routines for data cleaning","id":374,"level":3,"practice_id":39,"description":"use existing routines for data cleaning in order to avoid implementing everything from scratch","urls":["https://stackoverflow.com/questions/2066005","","",""]}],"id":219,"level":2},{"name":"Documentation/traceability","children":[{"name":"use separate files","id":375,"level":3,"practice_id":115,"description":"use separate files to save each model trained, in order to keep track of all the experiments","urls":["https://datascience.stackexchange.com/questions/56318","","",""]},{"name":"document params","id":376,"level":3,"practice_id":117,"description":"document the parameters used in training","urls":["https://datascience.stackexchange.com/questions/56318","","",""]}],"id":220,"level":2},{"name":"Consistency/Integrity","children":[{"name":"propagate data deletion in pipeline","id":377,"level":3,"practice_id":118,"description":"make sure you propagate data deletion across pipeline tasks when needed","urls":["https://datascience.stackexchange.com/questions/73163","","",""]}],"id":221,"level":2},{"name":"Resources usage","children":[{"name":"verify dataset/model fits in capacity","id":378,"level":3,"practice_id":119,"description":"verify that both model and data set fit in memory","urls":["https://datascience.stackexchange.com/questions/13901","https://softwareengineering.stackexchange.com/questions/421223","",""]},{"name":"use sparse structures for NLP","id":379,"level":3,"practice_id":134,"description":"when dealing with large corpus, in NLP related tasks, use sparse structures to improve the implementation performance ","urls":["https://datascience.stackexchange.com/questions/32859","","",""]},{"name":"use resource aware implementations","id":380,"level":3,"practice_id":114,"description":"use resource aware implementations when dealing with large data","urls":["https://datascience.stackexchange.com/questions/13901","https://softwareengineering.stackexchange.com/questions/421223","",""]},{"name":"understand efficiency of io","id":381,"level":3,"practice_id":111,"description":"think about input/output efcicincy when dealing with large datasets, large files,  parallel executions, GPU usage, etc","urls":["https://softwareengineering.stackexchange.com/questions/421223","","",""]},{"name":"parallelize training to optimize time","id":382,"level":3,"practice_id":112,"description":"if algorithms and data structures allow parallel execution try to use them to optimize the execution time","urls":["https://datascience.stackexchange.com/questions/13901","https://stackoverflow.com/questions/18567633","",""]}],"id":222,"level":2}],"id":104,"level":1},{"name":"Monitoring","children":[{"name":"Be aware of model performance and new data","children":[{"name":"observe data deviation from original data","id":383,"level":3,"practice_id":108,"description":"after model deployment, observe the new data needed as input for the model, in order to detect any deviation from the original data","urls":["https://datascience.stackexchange.com/questions/12761","","",""]},{"name":"avoid degradation retrain with new observation","id":384,"level":3,"practice_id":107,"description":"if new data is deviating from the original data distribution, you should retrain the model in order to avoid degradation","urls":["https://datascience.stackexchange.com/questions/12761","","",""]}],"id":223,"level":2}],"id":105,"level":1},{"name":"Labeling","children":[{"name":"Scalability","children":[{"name":"scale multiple humans","id":385,"level":3,"practice_id":109,"description":"data labeling should be done by more than one person, in order to scale the process ","urls":["https://datascience.stackexchange.com/questions/12344","","",""]}],"id":224,"level":2},{"name":"Parametrize","children":[{"name":"use tool that fixed aspect ratio for object detection","id":386,"level":3,"practice_id":110,"description":"in the case of object detection, use a tool for fixing the aspect ratio of the object that is going to be labeled","urls":["https://stackoverflow.com/questions/40476041","","",""]}],"id":225,"level":2}],"id":106,"level":1},{"name":"Deployment","children":[{"name":"Deployment","children":[{"name":"export entire pipeline","id":387,"level":3,"practice_id":120,"description":"when deploying a model, you should export the entire pipeline instead of only the model","urls":["https://stats.stackexchange.com/questions/400242","","",""]},{"name":"train best model on complete dataset","id":388,"level":3,"practice_id":122,"description":"deploy the model with the best hyper-parameters, but retrained with the entire dataset (i.e., training + validation + test sets)","urls":["https://stats.stackexchange.com/questions/400242","https://stats.stackexchange.com/questions/400242","https://stats.stackexchange.com/questions/11602",""]}],"id":226,"level":2}],"id":107,"level":1},{"name":"Requirement definition","children":[{"name":"Retraining model","children":[{"name":"Identify needs for model updating","id":389,"level":3,"practice_id":137,"description":"identify frequency  for model retraining","urls":["https://stackoverflow.com/questions/56859324","","",""]}],"id":227,"level":2},{"name":"External services","children":[{"name":"define pull&analize frequency based on requirements when using analytics cloud service","id":390,"level":3,"practice_id":121,"description":"when using an analylitics/ML cloud service define the service call frequency based on the use case and the model requirements","urls":["https://stackoverflow.com/questions/56859324","","",""]}],"id":228,"level":2},{"name":"Metric selection","children":[{"name":"select metrics based on goal","id":391,"level":3,"practice_id":86,"description":"to define how a model is going to be evaluated keep in mind the goal of the model (e.g., is it better to have a precise model?  Then precission may be the choosen metric, or do you want a precise model that also identifies the mayority of positives examples? then, maybe  metrics that balance precision and recall is a better one) ","urls":["https://datascience.stackexchange.com/questions/82012","","",""]}],"id":229,"level":2},{"name":"Probabilistic model","children":[{"name":"decouple probabilistic model optimization from probability threshold selection","id":392,"level":3,"practice_id":104,"description":"when using probabilistic forecasting in a decision system, decouple the  probabilistic model optimization from the probability threshold selection","urls":["https://stats.stackexchange.com/questions/405622","","",""]}],"id":230,"level":2},{"name":"Neural networks","children":[{"name":"use sigmoidal activation when function is differentiable","id":393,"level":3,"practice_id":143,"description":"when selecting an activation function for your neural network, you should be sure that the function is diferentiable","urls":["https://stackoverflow.com/questions/2785382","","",""]}],"id":231,"level":2},{"name":"","children":[{"name":"use relu activation to introduce no-linearity","id":394,"level":3,"practice_id":163,"description":"if you want to introduce no linearity to a neural network, try using relu as an activation function","urls":["https://stackoverflow.com/questions/45769058","","",""]}],"id":232,"level":2}],"id":108,"level":1},{"name":"Validation","children":[{"name":"Things to consider when evaluating a model","children":[{"name":"plot learning curves","id":395,"level":3,"practice_id":85,"description":"use learning curves to monitor the learning process evolution, which will help to identify issues in the model early","urls":["https://datascience.stackexchange.com/questions/13901","","",""]},{"name":"use crossvalidation when data insuficient to split in training validation testing","id":396,"level":3,"practice_id":94,"description":"use cross validation when data is insuficient to use training, validation, test split","urls":["https://stats.stackexchange.com/questions/355428","","",""]},{"name":"use nested-crossvalidation instead training test split","id":397,"level":3,"practice_id":95,"description":"use nested crossvalidation instead of training test split","urls":["https://stats.stackexchange.com/questions/410118","","",""]},{"name":"evaluate algorithms time vs accuracy","id":398,"level":3,"practice_id":79,"description":"compare models by measuring the time required to train each model and their accuracy","urls":["https://datascience.stackexchange.com/questions/13901","","",""]},{"name":"evaluate dataset samples time vs accuracy","id":399,"level":3,"practice_id":82,"description":"compare models by measuring the time required to train each model and their accuracy","urls":["https://datascience.stackexchange.com/questions/13901","","",""]},{"name":"use several runs of cross validation or bootstrap","id":400,"level":3,"practice_id":98,"description":"execute  cross validation times or boostrap ","urls":["https://stats.stackexchange.com/questions/355428","","",""]},{"name":"do not use hold out if crossvalidation is for verification","id":401,"level":3,"practice_id":75,"description":"do not use hold out when crossvalidation is used for verification (i.e., cross validation is not used for tuning hyper-parameters)","urls":["https://stats.stackexchange.com/questions/342110","","",""]},{"name":"select model based on test set","id":402,"level":3,"practice_id":87,"description":"when comparing more than one model, select the best one on the test set","urls":["https://stats.stackexchange.com/questions/179872","","",""]},{"name":"verify model is not biased when building superset vocabulary","id":403,"level":3,"practice_id":105,"description":"in NLP, when building a superset vocabulary (i.e., a vocabulary that is built with the whole dataset), check that the model is not biased ","urls":["https://datascience.stackexchange.com/questions/32859","","",""]},{"name":"use crossvalidation instead training test split","id":404,"level":3,"practice_id":93,"description":"use crossvalidation instead of training test split","urls":["https://stats.stackexchange.com/questions/410118","","",""]},{"name":"evaluate with nested-crossvalidation when desirable lower random uncertainty","id":405,"level":3,"practice_id":81,"description":"if you want to have low random uncertainty on the model verification, you could use nested crossvalidation","urls":["https://stats.stackexchange.com/questions/342110","","",""]},{"name":"penalize error in nested-crossvalidation","id":406,"level":3,"practice_id":178,"description":"penalize error in nested crossvalidation for those hyper-parameters that can lead to very-complex models","urls":["https://stats.stackexchange.com/questions/11602","","",""]},{"name":"check membership new data to training set","id":407,"level":3,"practice_id":103,"description":"when validating models, check membership of test data to training set (i.e., the distributions are different)","urls":["https://cs.stackexchange.com/questions/115441","","",""]},{"name":"use adversarial inputs for testing","id":408,"level":3,"practice_id":100,"description":"use adversarial inputs for testing a model, in order to ensure its robustness","urls":["https://datascience.stackexchange.com/questions/82805","","",""]},{"name":"do not compute r2 small sample","id":409,"level":3,"practice_id":77,"description":"do no compute R2 when having small samples","urls":["https://datascience.stackexchange.com/questions/77298","","",""]},{"name":"use vocabulary from training set when using crossvalidation","id":410,"level":3,"practice_id":91,"description":"when doing  crossvalidation on a NLP task, use the vocabulary built on the trainig set instead of  a vocabulary built on the whole dataset","urls":["https://datascience.stackexchange.com/questions/32859","","",""]},{"name":"use random seed to assure reproducibility and fair comparison when training deep neural network","id":411,"level":3,"practice_id":96,"description":"use a fixed seed when  neural networks training to ensure reproducibility and fair comparision","urls":["https://stackoverflow.com/questions/65706957","","",""]},{"name":"use repeated cross validation with different seed to avoid impact of fixed seed","id":412,"level":3,"practice_id":97,"description":"repeat crossvalidation with different seed to avoid the impact of using a fixed seed","urls":["https://stackoverflow.com/questions/65706957","","",""]},{"name":"use single cross validation and test with several seed to avoid seed impact","id":413,"level":3,"practice_id":99,"description":"repeat crossvalidation with different seed to avoid the impact of using a fixed seed","urls":["https://stackoverflow.com/questions/65706957","","",""]},{"name":"decouple probabilistic model optimization from probability threshold selection","id":414,"level":3,"practice_id":104,"description":"when using probabilistic forecasting in a decision system, decouple the  probabilistic model optimization from the probability threshold selection","urls":["https://stats.stackexchange.com/questions/405622","","",""]},{"name":"check stationary assumption with each prediction","id":415,"level":3,"practice_id":182,"description":"check that the stationarity assumption holds whit each prediction","urls":["https://cs.stackexchange.com/questions/115441","","",""]}],"id":233,"level":2},{"name":"Hyper parameter tuning","children":[{"name":"optimize hyperparameter in valdation set","id":416,"level":3,"practice_id":83,"description":"hyper-parameters  optimization should be done with a validation set","urls":["https://stats.stackexchange.com/questions/179872","","",""]},{"name":"optimize classifiers independently when using ensemble","id":417,"level":3,"practice_id":84,"description":"when building an ensemble, optimize hyper-parameters for each classifier independently","urls":["https://stats.stackexchange.com/questions/308376","","",""]},{"name":"evaluate model in test set after cross-validation for hyperparameter tuning","id":418,"level":3,"practice_id":80,"description":"after finding the best hyper-parameters, test the model with those hyper-parameters in the test set","urls":["https://stats.stackexchange.com/questions/342110","https://stackoverflow.com/questions/65706957","",""]},{"name":"test best hyperparameters on test set","id":419,"level":3,"practice_id":90,"description":"after finding the best hyper-parameters, test the model with those hyper-parameters in the test set","urls":["https://stats.stackexchange.com/questions/179872","","",""]},{"name":"calculate metrics at the end of epoch","id":420,"level":3,"practice_id":89,"description":"to monitor the evolution of the learning process when training neural networks, compute metrics at the end of  each epochs","urls":["https://stats.stackexchange.com/questions/425014","","",""]}],"id":234,"level":2},{"name":"Unit testing","children":[{"name":"use use annotated data for unit testing","id":421,"level":3,"practice_id":101,"description":"use previous annotated data for unit testing ","urls":["https://datascience.stackexchange.com/questions/82805","","",""]}],"id":235,"level":2},{"name":"Avoid overfitting","children":[{"name":"avoid overfitting bayesian optimization","id":422,"level":3,"practice_id":69,"description":"avoid overfitting by using bayesian optimization","urls":["https://datascience.stackexchange.com/questions/77298","","",""]},{"name":"avoid overfitting compare training test performance","id":423,"level":3,"practice_id":70,"description":"compare training  vs testing performance to avoid overfitting","urls":["https://datascience.stackexchange.com/questions/77298","","",""]},{"name":"avoid overfitting cross validation","id":424,"level":3,"practice_id":71,"description":"use cross validation to avoid overfitting","urls":["https://datascience.stackexchange.com/questions/12761","https://datascience.stackexchange.com/questions/13901","https://stackoverflow.com/questions/65706957",""]},{"name":"avoid overfitting nested cross validation","id":425,"level":3,"practice_id":72,"description":"use nested cross validation to avoid overfitting","urls":["https://stats.stackexchange.com/questions/15585","https://stats.stackexchange.com/questions/11602","",""]},{"name":"avoid overfitting model selection performed separately in each trial","id":426,"level":3,"practice_id":73,"description":"use nested cross validation to avoid overfitting","urls":["https://stats.stackexchange.com/questions/15585","","",""]},{"name":"avoid overfitting by using multiple randomised partitionings of the available data","id":427,"level":3,"practice_id":74,"description":"evaluate the model by using multiple randomised partitionings of the available data","urls":["https://stats.stackexchange.com/questions/15585","","",""]},{"name":"evaluate performance over a wide range of datasets","id":428,"level":3,"practice_id":78,"description":"calculate model performance over a wide range of datasets to avoid overfitting","urls":["https://stats.stackexchange.com/questions/15585","","",""]}],"id":236,"level":2}],"id":109,"level":1},{"name":"Training","children":[{"name":"Selecting... learning rates","children":[{"name":"reduce learning rate if it stagnates","id":429,"level":3,"practice_id":189,"description":"if a neural network stagnates with non optimal results, try to reduce the learning rate","urls":["https://datascience.stackexchange.com/questions/37163","","",""]},{"name":"use simulate annealing to reduce learning rate","id":430,"level":3,"practice_id":164,"description":"if a neural network stagnates with non optimal results, try to  use simulated annealing  ","urls":["https://datascience.stackexchange.com/questions/37163","","",""]}],"id":237,"level":2},{"name":"Retraining models","children":[{"name":"retrain with new data","id":431,"level":3,"practice_id":172,"description":"when using clustering, retrain models with new data to make more clusters and increase clustering efficiency ","urls":["https://datascience.stackexchange.com/questions/64990","https://datascience.stackexchange.com/questions/12765","",""]},{"name":"retrain offline if new data is not frequent","id":432,"level":3,"practice_id":173,"description":"retrain a model in ofline mode, i.e., retrain models by adding new data to the preexisting dataset. This method leads to a better global approximation, but it is unpractical for large datasets ","urls":["https://datascience.stackexchange.com/questions/12765","","",""]},{"name":"use online learning to train with new data","id":433,"level":3,"practice_id":165,"description":"retrain a model with online approximation, when your model needs to dynamically adapt to new patterns in data","urls":["https://stats.stackexchange.com/questions/467757","https://stackoverflow.com/questions/28006451","https://datascience.stackexchange.com/questions/12765",""]},{"name":" batch/minibatch based on application and model","id":434,"level":3,"practice_id":174,"description":"retrain a model with batch/mini batch approach, when you do not want an offline nor online approximation","urls":["https://datascience.stackexchange.com/questions/12765","","",""]},{"name":"use small data to react to concept drift","id":435,"level":3,"practice_id":158,"description":"when training model for data streams use small batches to adapt the model and avoid \\"concept drift\\"","urls":["https://stats.stackexchange.com/questions/297871","","",""]}],"id":238,"level":2},{"name":"Convergence","children":[{"name":"reduce learning rate by 1/2 or 1/3 if model has peaks on performance","id":436,"level":3,"practice_id":188,"description":"if a deep neural network does not converge, try to reduce the learning rate by a half or by  one third","urls":["https://stackoverflow.com/questions/43857303","","",""]},{"name":"increase iterations for model converging","id":437,"level":3,"practice_id":170,"description":"if a deep neural network does not converge, try to increase the number of iterations","urls":["https://stackoverflow.com/questions/43857303","","",""]},{"name":"increase mini batch to stabilize model","id":438,"level":3,"practice_id":171,"description":"if a deep neural network  does not converge, try to increase the size of the minibatch","urls":["https://stackoverflow.com/questions/43857303","","",""]},{"name":"compute derivates of cost function when objectives functions slow or do not converge","id":439,"level":3,"practice_id":88,"description":"when objective functions do not converge or converge slowly compute manually  the derivates of  the cost function","urls":["https://stats.stackexchange.com/questions/194833","","",""]},{"name":"use cost function","id":440,"level":3,"practice_id":92,"description":"when objective functions do not converge or converge slowly compute manually  the derivates of  the cost function","urls":["https://stats.stackexchange.com/questions/194833","","",""]},{"name":"use sgd back for MLP converge faster","id":441,"level":3,"practice_id":162,"description":"for faster convergence of MLPs use Stochastic Gradient Descent    ","urls":["https://stackoverflow.com/questions/45769058","","",""]}],"id":239,"level":2},{"name":"Improve performance","children":[{"name":"do not introduce unwanted confounding between origins and seasonality","id":442,"level":3,"practice_id":76,"description":"use subsamplig for forecasting time series but make suere that you do not introduce  unwanted confounding between origins and seasonality","urls":["https://stats.stackexchange.com/questions/175714","","",""]},{"name":"subsample when using rolling origin on large dataset","id":443,"level":3,"practice_id":102,"description":"when forecasting large time series use the rolling origin technique","urls":["https://stats.stackexchange.com/questions/175714","","",""]},{"name":"use adversarial input to ensure robustness","id":444,"level":3,"practice_id":138,"description":"for model robustness train models use adversial machine learning","urls":["https://ai.stackexchange.com/questions/16756","","",""]},{"name":"use subsampling","id":445,"level":3,"practice_id":159,"description":"when training a large time series, if performance is  impacted then subsample data","urls":["https://stats.stackexchange.com/questions/175714","","",""]}],"id":240,"level":2},{"name":"When using transfer learning","children":[{"name":"use flatten layer decrease computational cost","id":446,"level":3,"practice_id":140,"description":"when you use transfer learning, remove the flattening layer of a model in order to decrease the computational cost of fine-tunning the model","urls":["https://stackoverflow.com/questions/58644114","","",""]},{"name":"use fine tunning when not sufficient samples for training a model","id":447,"level":3,"practice_id":151,"description":"if you are working with a deep neural network and you do not have enough data to train a deep model from scratch,  try using transfer learning with fine tuning","urls":["https://stackoverflow.com/questions/32966970","","",""]},{"name":"use pretrained models","id":448,"level":3,"practice_id":161,"description":"if you are working with a deep neural network and you do not have enough data to train a deep model from scratch,  try using transfer learning with fine tuning","urls":["https://stackoverflow.com/questions/32966970","","",""]}],"id":241,"level":2},{"name":"Things to consider when training a model","children":[{"name":"do not include test data","id":449,"level":3,"practice_id":177,"description":"do not include test data for training in order to have a fair metric ","urls":["https://stats.stackexchange.com/questions/482486","","",""]},{"name":"train using sliding window in time series","id":450,"level":3,"practice_id":185,"description":"when using time series data, consider training models with sliding windows ","urls":["https://stats.stackexchange.com/questions/175714","","",""]},{"name":"scale invariant algorithms when having mixed data","id":451,"level":3,"practice_id":41,"description":"if you are working with data that has multiple features which are not in the same scale, try to use scale invariant-algorithms (e.g., tree based algorithms)  ","urls":["https://datascience.stackexchange.com/questions/89696","","",""]},{"name":"use gradient boosting trees when obligatory features exists tree-based algorithms","id":452,"level":3,"practice_id":147,"description":"when training a model and having dependencies between features, use a gradient boosting tree instead of a decision tree or a random forest","urls":["https://datascience.stackexchange.com/questions/1094","","",""]},{"name":"use different models for data splits in regressions when data is dependable on categorical data","id":453,"level":3,"practice_id":166,"description":"train a specific model for each category of a categorical feature when you want to build specialized models based on particular categories (e.g., train a regression model  to predict the car prices for  Asia and and another model to predict car prices in Europe)","urls":["https://stats.stackexchange.com/questions/191348","","",""]},{"name":"optimize probabilistic model then deriving decisions from it separately","id":454,"level":3,"practice_id":187,"description":"when building a probabilistic model in a decision system, optimize it in a first place and then you can derive decisions from it separately","urls":["https://stats.stackexchange.com/questions/405622","","",""]},{"name":"use as many data as possible","id":455,"level":3,"practice_id":160,"description":"when trainning a model, use as many data as possible","urls":["https://stats.stackexchange.com/questions/297875","","",""]},{"name":"use mixed models when having repeated measures","id":456,"level":3,"practice_id":168,"description":"when having repeated measures train mixed models","urls":["https://stats.stackexchange.com/questions/311422","","",""]}],"id":242,"level":2},{"name":"Things to consider when training a neural network","children":[{"name":"use dropout layers","id":457,"level":3,"practice_id":149,"description":"when training a neural network use drop out layer(s) to avoid overfitting","urls":["https://stackoverflow.com/questions/43856384","https://stackoverflow.com/questions/58644114","",""]},{"name":"use dropout to avoid overfitting","id":458,"level":3,"practice_id":150,"description":"when training a neural network use drop out layer(s) to avoid overfitting","urls":["https://stackoverflow.com/questions/58644114","https://stackoverflow.com/questions/32966970","",""]},{"name":"use online(batch=1) to adapt quickly to new data","id":459,"level":3,"practice_id":155,"description":"when retraining a neural network use a training batch with size equals to 1 (i.e., online learning) to quickly adapt the model to new data","urls":["https://datascience.stackexchange.com/questions/12765","","",""]},{"name":"use online(batch=1) training when having large data set","id":460,"level":3,"practice_id":156,"description":"when retraining online a neural network and having a large dataset use a training batch with size equals to 1","urls":["https://datascience.stackexchange.com/questions/12765","","",""]},{"name":"use fully convolutional network to deal with images","id":461,"level":3,"practice_id":145,"description":"when training a neural network model and you are working with images, prefer a fully convolutional layer than a fully connected one.","urls":["https://stats.stackexchange.com/questions/450246","","",""]},{"name":"use number samples proportional to number parameters","id":462,"level":3,"practice_id":167,"description":"when training a deep neural network model, the number of instances needed for training is proportional to the number of parameters","urls":["https://stackoverflow.com/questions/32966970","","",""]},{"name":"use wavenet for long sequences","id":463,"level":3,"practice_id":144,"description":"when having long sequences use wavennet instead of LSTM","urls":["https://stackoverflow.com/questions/40918978","","",""]},{"name":"use early stopping","id":464,"level":3,"practice_id":139,"description":"when training neural networks, use early stopping. i.e., monitor on  regular basis the model quality  against a validation set, and stop it when a termination condition is satisfied (e.g., after 10 epochs the model has not improved in 0.001 in accuracy)","urls":["https://stats.stackexchange.com/questions/425014","","",""]},{"name":"use more layers in neural network when having high variance data","id":465,"level":3,"practice_id":141,"description":"when training a neural network and having high variance data increase the number of  layers in the model","urls":["https://stackoverflow.com/questions/58644114","","",""]},{"name":"initialize neural network components with differrent values","id":466,"level":3,"practice_id":169,"description":"when training a neural network it is important to initialize their components with different values to avoid symmetry","urls":["https://stackoverflow.com/questions/65704588","","",""]},{"name":"use random initialization with neural network to break simmetry","id":467,"level":3,"practice_id":157,"description":"e.g., for breaking symmetry in neural networks initialize weights randomly","urls":["https://stackoverflow.com/questions/65704588","","",""]},{"name":"use kernels to predict geo data with geodesic distance","id":468,"level":3,"practice_id":154,"description":"use geodesic distances as the metric for kernels  when doing preditions with geodata","urls":["https://stats.stackexchange.com/questions/207566","","",""]},{"name":"train neural network many times with different weights and compute average results and variance","id":469,"level":3,"practice_id":184,"description":"train multiple times the same neural network with different weights, in order to be able to get a more specific  judgement, by computing the average performance and its variance ","urls":["https://stackoverflow.com/questions/65704588","","",""]},{"name":"generate mutiple sequences when training set invariant to permutations","id":470,"level":3,"practice_id":186,"description":"when training a model to classify sentences in which the order of words is not relevant   (e.g.,  if you use as input the following words sequence in any order,\\"The president gave up to build a wall\\", any permutation  should be classified as politics),  train a LSTM multiple times with the same sentences but  shufling its order or  sample elements randomly  from the sentence,","urls":["https://stats.stackexchange.com/questions/394607","","",""]},{"name":"use deep set to train models with set invatiant to permutations","id":471,"level":3,"practice_id":152,"description":"when training a model to classify sentences in which the order of words is not relevant   (e.g.,  if you use as input the following words sequence in any order,\\"The president gave up to build a wall\\", any permutation  should be classified as politics),  use deep sets","urls":["https://stats.stackexchange.com/questions/394607","","",""]}],"id":243,"level":2},{"name":"Ensembling","children":[{"name":"ensemble independent models","id":472,"level":3,"practice_id":179,"description":"in an ensemble use only the models that are independent","urls":["https://stats.stackexchange.com/questions/308376","","",""]},{"name":"ensemble independent models when having time series data","id":473,"level":3,"practice_id":180,"description":"when training mixed type features including time series data, use ensemble methods","urls":["https://stackoverflow.com/questions/18853557","","",""]},{"name":"check correlation before ensembling models","id":474,"level":3,"practice_id":181,"description":"before ensembling models, check the correlation of the models prediction","urls":["https://stats.stackexchange.com/questions/308376","","",""]},{"name":"split data among learners ensemble","id":475,"level":3,"practice_id":175,"description":"when having a large dataset that does not fit in memory, train multiple models on random data samples, then build an ensemble with those models in order to get a more accurate model (i.e., the ensemble)  than a base learner with fewer data.","urls":["https://datascience.stackexchange.com/questions/13901","","",""]}],"id":244,"level":2},{"name":"Un-learn","children":[{"name":"un-learn data with instance based learning","id":476,"level":3,"practice_id":176,"description":"use instance based learning algorithms when un-learning data is a posibility","urls":["https://datascience.stackexchange.com/questions/73163","","",""]}],"id":245,"level":2},{"name":"DQN","children":[{"name":"use mellow max policy with DQN instead of Boltzmann Exploration","id":477,"level":3,"practice_id":146,"description":"when training a DQN model use mellow max policy instead of Boltzman Exploration because it provides an adaptive temperature for Boltzmann exploration.","urls":["https://ai.stackexchange.com/questions/22064","","",""]}],"id":246,"level":2}],"id":110,"level":1}]}')},223:function(e,t,i){},241:function(e,t,i){},242:function(e,t,i){"use strict";i.r(t);var a,s,n,o,r,c,d=i(0),l=i.n(d),u=(i(219),i(28)),h=i.n(u),m=(i(223),i(5)),p=i(302),g=i(303),f=i(48),v=i(300),w=i(23),b=i(37),y=b.a.div(a||(a=Object(w.a)(["\n  display: flex;\n  flex-direction: column;\n"]))),k=b.a.div(s||(s=Object(w.a)(["\n  width: 460px;\n  overflow-wrap: break-word;\n"]))),x=i(1),q=function(e){var t=e.show,i=e.setShow,a=e.node;return Object(x.jsx)(v.a,{show:t,onHide:function(){return i(!1)},children:a&&Object(x.jsxs)(x.Fragment,{children:[Object(x.jsx)(v.a.Header,{children:Object(x.jsx)(v.a.Title,{children:Object(x.jsx)(k,{children:a.name})})}),Object(x.jsx)(v.a.Body,{children:Object(x.jsxs)(y,{children:[Object(x.jsxs)("h6",{children:["ID ",a.practice_id]}),Object(x.jsxs)("h6",{children:["Origin: ",a.origin]}),Object(x.jsx)("p",{children:a.description}),Object(x.jsx)("h6",{children:"URLs"}),a.urls&&a.urls.map((function(e,t){return Object(x.jsx)("a",{href:e,children:e},t)}))]})})]})})},_=1500,j=10,z=10,O=-30,S=[O,-j,_,28],T=function(e,t){e.each((function(){for(var e,i=f.c(this),a=i.text().split(/\s+/).reverse(),s=[],n=0,o=i.attr("x"),r=i.attr("y"),c=i.text(null).append("tspan").attr("x",o).attr("y",r);e=a.pop();)s.push(e),c.text(s.join(" ")),c.node().getComputedTextLength()>t&&(s.pop(),c.text(s.join(" ")),s=[e],c=i.append("tspan").attr("x",o).attr("y",10*++n+r).text(e))}))},D=function(e){var t,i=0;for(t=0;t<e.length;t++)i=e.charCodeAt(t)+((i<<4)-i);var a="#";for(t=0;t<3;t++){a+=("00"+(i>>4*t&255).toString(26)).substr(-2)}return a},E=function(e,t){3===(e=e.replace(/^\s*#|\s*$/g,"")).length&&(e=e.replace(/(.)/g,"$1$1"));var i=parseInt(e.substr(0,2),16),a=parseInt(e.substr(2,2),16),s=parseInt(e.substr(4,2),16);return"#"+(0|256+i+(256-i)*t/100).toString(16).substr(1)+(0|256+a+(256-a)*t/100).toString(16).substr(1)+(0|256+s+(256-s)*t/100).toString(16).substr(1)},M=function(e){var t=e.data,i=l.a.useState(!1),a=Object(m.a)(i,2),s=a[0],n=a[1],o=l.a.useState(),r=Object(m.a)(o,2),c=r[0],d=r[1],u=l.a.useState(S),h=Object(m.a)(u,2),p=h[0],g=h[1],v=function(e,t){var i=l.a.useRef();return l.a.useEffect((function(){return e(f.c(i.current)),function(){}}),t),i}((function(e){var i=f.d().nodeSize([28,375]),a=f.a(t),s=f.b().x((function(e){return e.y})).y((function(e){return e.x}));a.x0=375,a.y0=0,a.descendants().forEach((function(e,t){e.id=t,e._children=e.children,e.depth>1&&(e.children=null)}));var o=e.append("g").attr("fill","none").attr("stroke","black").attr("stroke-opacity",.5).attr("stroke-width",1.5),r=e.append("g").attr("cursor","pointer").attr("pointer-events","all");!function t(c){var l=a.descendants().reverse(),u=a.links();i(a);var h=a,m=a;a.eachBefore((function(e){e.x<h.x&&(h=e),e.x>m.x&&(m=e)}));var p=m.x-h.x+j+z,f=e.transition().duration(250).attr("viewBox",[O,h.x-j,_,p]).tween("resize",window.ResizeObserver?null:function(){return function(){return e.dispatch("toggle")}}),v=r.selectAll("g").data(l,(function(e){return e.id})),w=v.enter().append("g").attr("transform",(function(e){return"translate(".concat(c.y0,",").concat(c.x0,")")})).attr("fill-opacity",0).attr("stroke-opacity",0).on("click",(function(i,a){0!==a.depth&&(a.children=a.children?null:a._children,t(a),function(t){var i=e.attr("viewBox").split(",");3===t.data.level&&(n(!0),g([Number(i[0]),Number(i[1]),Number(i[2]),Number(i[3])]),d(t.data))}(a))}));w.append("rect").attr("width",20).attr("height",15).attr("rx",5).attr("ry",5).attr("y",-10).attr("fill",(function(e){return function(e){if("Start"===e.data.name)return"#262626";if(e._children||3===e.data.level){var t=e.parent,i=t.parent,a=null!==i?"Start"===i.data.name?t.data.name:i.data.name:"Start"===t.data.name?e.data.name:t.data.name,s=D(a);return 1===e.data.level?E(s,20):2===e.data.level?E(s,40):3===e.data.level?E(s,60):"gray"}return"gray"}(e)})),w.append("text").attr("x",(function(e){return-2})).attr("text-anchor",(function(e){return"end"})).style("font-size","12px").text((function(e){return e.data.name})).call(T,200).clone(!0).lower().attr("stroke-linejoin","round").attr("stroke-width",2).attr("stroke","white"),v.merge(w).transition(f).attr("transform",(function(e){return"translate(".concat(e.y,",").concat(e.x,")")})).attr("fill-opacity",1).attr("stroke-opacity",1),v.exit().transition(f).remove().attr("transform",(function(e){return"translate(".concat(c.y,",").concat(c.x,")")})).attr("fill-opacity",0).attr("stroke-opacity",0);var b=o.selectAll("path").data(u,(function(e){return e.target.id})),y=b.enter().append("path").attr("d",(function(e){var t={x:c.x0,y:c.y0-30};return s({source:t,target:t})}));b.merge(y).transition(f).attr("d",s),b.exit().transition(f).remove().attr("d",(function(e){var t={x:c.x,y:c.y};return s({source:t,target:t})})),a.eachBefore((function(e){e.x0=e.x,e.y0=e.y}))}(a)}),[t.length]);return Object(x.jsxs)(x.Fragment,{children:[Object(x.jsx)(q,{show:s,node:c,setShow:n}),Object(x.jsx)("div",{style:{height:"100%",width:_},children:Object(x.jsx)("svg",{ref:v,viewBox:[p],style:{height:1600,width:"100%",userSelect:"none"}})})]})},C=i(118),N=i.n(C),P=b.a.div(n||(n=Object(w.a)(["\n  text-align: left;\n"]))),L=function(e){var t=e.data,i=e.onBackClick;return Object(x.jsxs)(P,{children:[Object(x.jsx)(N.a,{onClick:i,sx:{color:"#2f6ed4"}}),Object(x.jsx)(M,{data:t})]})},R=i(299),I=b.a.div(o||(o=Object(w.a)(["\n  height: ","px;\n  padding: 20px;\n"])),(function(e){return e.height})),B=b.a.div(r||(r=Object(w.a)(["\n  .MuiDataGrid-columnHeaderTitle {\n    overflow: unset;\n    text-overflow: unset;\n    white-space: normal;\n    line-height: initial;\n    font-weight: 600;\n  }\n  \n  .MuiDataGrid-columnHeaderTitleContainer{\n    white-space: normal;\n  }\n  \n  .MuiDataGrid-columnHeaders{\n    max-height: fit-content !important;\n    line-height: unset !important;\n  }\n  \n  .MuiDataGrid-row{\n    height: auto !important;\n    max-height: 100% !important;\n  }\n  \n  .MuiDataGrid-cell {\n    max-height: 100% !important;\n    white-space: break-spaces !important;\n  }\n  \n  .MuiDataGrid-cellContent {\n    overflow-wrap: break-word !important;\n  }\n  \n  .MuiDataGrid-virtualScroller {\n    overflow-y: scroll !important;\n  }\n"]))),A=function(e){var t=e.columns,i=e.data,a=e.tableHeight,s=void 0===a?400:a;return Object(x.jsx)(B,{children:Object(x.jsx)(I,{height:s,children:Object(x.jsx)(R.a,{rows:i,columns:t,hideFooter:!0,disableSelectionOnClick:!0,disableColumnMenu:!0,rowHeight:60})})})},G=[{id:"Mr1","Phase Category":"Requirement definition",Subcategory:"Metric selection",Description:"The goal of the model that is going to be trained should be considered when selecting the most appropriate metrics for assessing the model performance.","Number of experts that validated the practice":4,"STE Post(s)":"https://datascience.stackexchange.com/questions/82012","External URL(S) associated to the post":"https://imbalanced-learn.org/stable/"},{id:"Mr2","Phase Category":"Requirement definition",Subcategory:"Retraining model",Description:"It is important to identify the needs/requirements of model retraining.","Number of experts that validated the practice":4,"STE Post(s)":"https://stackoverflow.com/questions/56859324","External URL(S) associated to the post":"-"},{id:"Mr3","Phase Category":"Requirement definition",Subcategory:"External services",Description:"If an ML model is published as a cloud service, specifically, when a classification/prediction task uses ML cloud-based services, it is important to define the use case and the model requirements in order to identify how frequently the service should be invoked.","Number of experts that validated the practice":4,"STE Post(s)":"https://stackoverflow.com/questions/56859324","External URL(S) associated to the post":"-"},{id:"Mr4","Phase Category":"Requirement definition",Subcategory:"Probabilistic model",Description:"When using probabilistic forecasting in a decision system, it is necessary to decouple the probabilistic model optimization from the probability threshold selection.","Number of experts that validated the practice":3,"STE Post(s)":"https://stats.stackexchange.com/questions/405622","External URL(S) associated to the post":"https://stats.stackexchange.com/questions/390186/is-decision-threshold-a-hyperparameter-in-logistic-regression\nhttps://stats.stackexchange.com/a/405049/1352\nhttps://www.fharrell.com/post/classification/"}],F=[{field:"id",headerName:"ID",width:160},{field:"Phase Category",headerName:"Phase Category",width:200},{field:"Subcategory",headerName:"Subcategory",width:160},{field:"Description",headerName:"Description",width:300},{field:"Number of experts that validated the practice",headerName:"Number of experts that validated the practice",width:200,type:"number"},{field:"STE Post(s)",headerName:"STE Post(s)",width:250},{field:"External URL(S) associated to the post",headerName:"External URL(S) associated to the post",width:300}],U=b.a.div(c||(c=Object(w.a)(["\n  padding: 0 40px;\n"]))),H=function(e){var t=e.data,i=d.useState(!1),a=Object(m.a)(i,2),s=a[0],n=a[1];return Object(x.jsx)(U,{children:s?Object(x.jsx)(L,{data:t,onBackClick:function(){return n(!1)}}):Object(x.jsxs)(x.Fragment,{children:[Object(x.jsx)(g.a,{variant:"h3",align:"center",margin:10,children:"What are the Machine Learning best practices reported by practitioners on Stack Exchange?"}),Object(x.jsx)(g.a,{variant:"h4",children:"Taxonomy"}),Object(x.jsx)(p.a,{onClick:function(){return n(!0)},children:"Open chart"}),Object(x.jsx)("br",{}),Object(x.jsx)("br",{}),Object(x.jsx)(g.a,{variant:"h4",children:"Complementary tables of practices"}),Object(x.jsx)(A,{data:G,columns:F,tableHeight:500})]})})},J=i(124);i(241);var W=function(){return Object(x.jsx)("div",{className:"App",children:Object(x.jsx)(H,{data:J})})},Q=function(e){e&&e instanceof Function&&i.e(3).then(i.bind(null,304)).then((function(t){var i=t.getCLS,a=t.getFID,s=t.getFCP,n=t.getLCP,o=t.getTTFB;i(e),a(e),s(e),n(e),o(e)}))};h.a.render(Object(x.jsx)(l.a.StrictMode,{children:Object(x.jsx)(W,{})}),document.getElementById("root")),Q()}},[[242,1,2]]]);
//# sourceMappingURL=main.5d4692e3.chunk.js.map